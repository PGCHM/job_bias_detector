{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8555cdef-cd99-46ea-a104-6834fed4c1e5",
   "metadata": {},
   "source": [
    "<img src=\"img/women-profile-international-woman-day-hair-vector-49155885.avif\" alt=\"Yes Women Can!\" />\n",
    "\n",
    "# Job Bias Analyzer: Promoting Gender Equality Through AI-Powered Job Description Analysis\n",
    "\n",
    "**Author:** [Chunming Peng](https://github.com/PGCHM)\n",
    "\n",
    "**GitHub Repo:** https://github.com/PGCHM/job_bias_detector\n",
    "\n",
    "**Banner Image Provided by** [Vector Stock](https://www.vectorstock.com/royalty-free-vector/)\n",
    "\n",
    "The Job Bias Analyzer is an innovative AI-powered tool designed to identify and eliminate discriminatory language and implicit bias in job descriptions and hiring materials. By leveraging natural language processing and machine learning techniques, this tool actively contributes to achieving UN Sustainable Development Goal 5 (Gender Equality) by promoting equal opportunities in the workforce through inclusive job postings. This tool could help companies identify and eliminate unintentional biases in recruitment materials, promoting a more inclusive hiring process. Google’s Gemini APIs are used to detect and suggest gender-neutral language.\n",
    "\n",
    "\n",
    "## Needs, Impacts and Relevance\n",
    "\n",
    "The project tackles a critical issue in workforce equality: unconscious bias in job descriptions that can discourage women and LGBTQ groups from applying for positions. Research shows that gendered language in job postings can reduce women's application rates by up to 40%. This tool directly addresses UN SDG target 5.1 (ending discrimination against women) and 5.5 (ensuring equal opportunities in economic life) by helping organizations create more inclusive job postings.\n",
    "\n",
    "As for potential for positive impact, the tool offers immediate and scalable impact through (1) Real-time analysis of job descriptions, (2) Specific recommendations for bias-free alternatives, (3) Educational feedback that helps writers understand why certain terms are problematic, (4) Database-backed learning system that improves recommendations over time, (5) Easy integration into existing hiring workflows, and (6) Cross-platform accessibility through command-line interface.\n",
    "\n",
    "In alignment with Women's Empowerment, the analyzer promotes gender equality by: (a) Identifying and eliminating gender-coded language, (b) Challenging traditional role stereotypes in job requirements, (c) Promoting inclusive leadership opportunities, (d) Addressing intersectional biases related to age, race, and other characteristics, and (e) Supporting organizational diversity and inclusion initiatives.\n",
    "\n",
    "\n",
    "## Project Introduction\n",
    "\n",
    "The AI-Powered Discrimination Detector for Job Listings focuses on using natural language processing (NLP) to identify potentially discriminatory or gender-biased language in job descriptions. Here’s a basic workflow it supports:\n",
    "\n",
    "1. Data Collection: Collect job listings from various sources.\n",
    "\n",
    "\n",
    "2. Bias Identification: Use NLP to flag potentially biased words or phrases that may discourage diverse candidates, particularly women.\n",
    "\n",
    "\n",
    "3. Suggesting Alternatives: Offer recommendations for neutral, inclusive language to replace flagged phrases.\n",
    "\n",
    "\n",
    "4. Feedback Loop: Gather feedback from users (HR professionals, job seekers) to improve detection accuracy and recommendations.\n",
    "\n",
    "\n",
    "Please see the workflow chart below for the modules and how they interact with each other:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65533e06-0a48-43ff-b6bc-8cf5058ff2e2",
   "metadata": {},
   "source": [
    "<img src=\"img/workflow.png\" alt=\"workflow chart\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195b65e-116f-4bf8-bdb0-e62cf7fbc3e6",
   "metadata": {},
   "source": [
    "## Methodologies\n",
    "\n",
    "Google Gemini AI, Google's new foundation model, excels in language processing tasks and can aid in analyzing and generating text to enhance inclusivity. Here’s a breakdown of how we proceed to build this tool with Google Gemini APIs:\n",
    "\n",
    "1. Define the Types of Bias to Detect\n",
    "\n",
    " - Gendered words (e.g., `ninja`, `rockstar` vs. `team player`, `leader`)\n",
    " - Language implying age or experience bias (e.g., `young, energetic team`)\n",
    " - Requirements that might exclude certain demographics (e.g., unnecessary physical requirements)\n",
    " - Gender pronouns or assumptions (e.g., `he/she` instead of `they`)\n",
    "\n",
    "\n",
    "2. Data Preparation and Annotation\n",
    "\n",
    " - Dataset: Start with publicly available job listings or scrape from job portals. You could also use synthetic examples if live data is hard to obtain.\n",
    " - Labeling: Label phrases or words associated with potential biases. Common NLP labeling platforms, such as Google’s Data Labeling Service, could help create training data if your project scales up.\n",
    "\n",
    "\n",
    "3. Leverage Google Gemini AI for Initial Analysis\n",
    "\n",
    " - Text Embedding and Analysis: Use Gemini’s text embeddings to analyze language. Embeddings capture the context and sentiment in words, helping you identify subtle biases.\n",
    " - Fine-Tuning: Fine-tune Gemini with your labeled dataset to detect specific biases in job descriptions. Google’s AI Platform offers tools for fine-tuning to capture nuances in biased language.\n",
    "\n",
    "\n",
    "4. Flagging and Recommending Inclusive Language\n",
    "\n",
    " - Bias Detection: Use Gemini to build a model that can score job postings for inclusivity based on predefined keywords and contextual analysis.\n",
    " - Generate Recommendations: For flagged terms, use Gemini’s language generation capabilities to suggest alternatives. For example, change `aggressive salesperson` to `results-driven salesperson`.\n",
    " - Implementation: build a simple feedback system that allows HR users to mark suggestions as `helpful` or `not helpful`, refining the model’s suggestions over time.\n",
    "\n",
    "\n",
    "5. Deploy and Create an Easy-to-Use Interface\n",
    "\n",
    " - Web App or API: Consider building a web application or API. Using Google Cloud’s App Engine or Firebase could simplify deployment.\n",
    " - User Dashboard: Create a dashboard for users to paste in job descriptions and view flagged terms with recommendations.\n",
    "\n",
    "\n",
    "### Usability\n",
    "\n",
    "In terms of User Experience, the tool prioritizes usability through:\n",
    "\n",
    " - Clear, intuitive command-line interface\n",
    " - Step-by-step guidance for users\n",
    " - Visual representation of bias severity\n",
    " - Detailed explanations for flagged terms\n",
    " - Multiple options for viewing and managing results\n",
    " - Toggle-able debug mode for technical users\n",
    "\n",
    "### Techinical Implementation\n",
    "\n",
    "The solution implements sophisticated technical features:\n",
    "\n",
    " - Asynchronous processing for responsive analysis\n",
    " - Persistent storage of feedback data for continuous improvement\n",
    " - Cross-platform compatibility\n",
    " - Modular design for easy expansion and maintenance\n",
    " - Clear separation of concerns between UI and analysis logic\n",
    " \n",
    "### Key functional features \n",
    "\n",
    "In terms of functionality and performance, the tool has included:\n",
    "\n",
    " - Rapid analysis of job descriptions of any length\n",
    " - Detailed scoring system for bias assessment\n",
    " - Specific, actionable recommendations for improvements\n",
    " - Persistent storage of feedback for continuous improvement\n",
    " - Error handling and recovery mechanisms\n",
    " - Cross-platform compatibility\n",
    " \n",
    " \n",
    "## Measurement of Success\n",
    "\n",
    "In order to place the metrics to measure the effectiveness of the tool, so we can compare whether each version of this tool is enhancing from the previous, the proposed metrics would be measured through:\n",
    "\n",
    " - Reduction in biased language in job postings\n",
    " - Increased diversity in job applicant pools\n",
    " - Improved gender balance in hiring outcomes\n",
    " - User feedback and satisfaction ratings\n",
    " - Adoption rates among organizations\n",
    " - Long-term impact on workplace diversity metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77921ac-bc30-4b57-9a0c-64a1e3fb13fa",
   "metadata": {},
   "source": [
    "## Example Use Cases and Outputs\n",
    "\n",
    "Here's a look at what this tool might detect and suggest:\n",
    "\n",
    "1. Detection:\n",
    "\n",
    "Job Description: `We need a young, energetic salesperson.`\n",
    "\n",
    "Flagged Terms: `young, energetic`\n",
    "\n",
    "Suggestion: Replace with `motivated, proactive`\n",
    "\n",
    "\n",
    "\n",
    "2. Detection:\n",
    "\n",
    "Job Description: `We are looking for a sales ninja who will crush targets!`\n",
    "\n",
    "Flagged Terms: `ninja, crush targets`\n",
    "\n",
    "Suggestion: Replace with `experienced sales professional with a strong record of achieving targets`\n",
    "\n",
    "\n",
    "\n",
    "3. Detection:\n",
    "\n",
    "Job Description: `We need someone willing to work long hours in a fast-paced environment.`\n",
    "\n",
    "Flagged Terms: `long hours`\n",
    "\n",
    "Suggestion: Replace with `flexible schedule based on project needs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9288f-25a7-47ca-9f59-8d792ec1f046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b38f8f79-b85d-414f-aaa9-2eb61113d38d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Resources and Tools\n",
    "\n",
    "1. Gemini Language API: For NLP and text generation, providing recommendations for inclusivity in job postings.\n",
    "\n",
    "\n",
    "2. Google Cloud AI Platform: For training and fine-tuning the model if you’re building it on labeled job description data.\n",
    "\n",
    "\n",
    "3. Google App Engine / Firebase: To deploy the web app or API.\n",
    "\n",
    "\n",
    "4. Data Labeling Service: To assist in creating labeled datasets to fine-tune Gemini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf36acc-bd62-43de-b94c-555384a368b0",
   "metadata": {},
   "source": [
    "The Python SDK for the Gemini API is contained in the google-generativeai package.\n",
    "\n",
    "Install the dependency using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31fad5-7d0c-40fd-9123-9b3606bc8401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aa765-11bf-4b41-96ba-37c451be6997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48031df5-2934-42a7-b04e-bd61c9e948de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install aiohttp  # if using async HTTP requests\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc2a89-ccd2-48ee-972c-8e303e784b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5bb9e-7f31-42b8-94e8-b6aa82a3a5f5",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "We want to create this AI tool that will be composed of these functionalities:\n",
    "\n",
    " - a Python script that can generate a detailed report identifying biased language, suggesting improvements, and providing a bias-free version of the job description. The report will be both printed to console and saved to a file.\n",
    " - a Simple UI component that can have users (e.g., HR) paste in the job description, identify any potential biases, recommend the fixes, and have users review whether this is helpful or not.\n",
    " - a Feedback loop that collects users' reviews, save into database(s), and feed back into the AI tool when predicting for next prompt.\n",
    "\n",
    "\n",
    "The Python script `job_bias_detector_args.py` provides:\n",
    "\n",
    " - A `JobBiasDetector` class that:\n",
    "   - Initializes the Gemini AI model\n",
    "   - Creates structured prompts for analysis\n",
    "   - Analyzes job descriptions for bias\n",
    "   - Generates detailed reports\n",
    " - Key features:\n",
    "   - Detects discriminatory language related to age, gender, and other protected characteristics\n",
    "   - Provides specific term-by-term analysis with suggestions\n",
    "   - Assigns a discrimination score (0-10) and confidence level\n",
    "   - Generates an improved, bias-free version of the description\n",
    "   - Creates detailed reports that can be saved to files\n",
    " - The analysis includes:\n",
    "   - Flagged problematic terms\n",
    "   - Explanations of why terms are problematic\n",
    "   - Suggested replacements\n",
    "   - Overall discrimination score\n",
    "   - Confidence level of the analysis\n",
    "   - Complete rewritten job description\n",
    "\n",
    "\n",
    "To break into details, the AI tool has been accomplished with these steps:\n",
    "\n",
    "\n",
    "### Step 0: Initialize the Google AI Platform (with Google Gemini)\n",
    "\n",
    "Import the package and configure the service with your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddea1c4-3c20-4c4c-bb48-662d24ddcd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key='YOUR_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a25ccf-8bd3-4065-ae7d-d9f5a587d885",
   "metadata": {},
   "source": [
    "Alternatively, use the `OAuth 2.0 Client IDs` stored in the `client_secret.json`, and call `load_creds` to generate a valid token when a `token.json` not exists or expires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919069fe-5da3-4941-8540-3fd7a6b3a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                client_secret, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open(token_json, 'w') as token:\n",
    "            token.write(creds.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e8287-33d9-4d3e-af64-cb23efceea09",
   "metadata": {},
   "source": [
    "### Step 1: Set up the disctionary of biased terms \n",
    "\n",
    "Example list of biased words/phrases and their inclusive alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570af995-16b4-40b9-aa76-509cd8541a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_dict = {\n",
    "            \"young\": {\n",
    "                \"categories\": [\"age discrimination\", \"direct discrimination\"],\n",
    "                \"replacement\": \"motivated\",\n",
    "                \"explanation\": \"Directly discriminates against older workers and violates age discrimination laws\"\n",
    "            },\n",
    "            \"energetic\": {\n",
    "                \"categories\": [\"age discrimination\", \"indirect discrimination\"],\n",
    "                \"replacement\": \"enthusiastic\",\n",
    "                \"explanation\": \"Often used as coded language for age discrimination and may discourage older applicants\"\n",
    "            },\n",
    "            \"ninja\": {\n",
    "                \"categories\": [\"unprofessional language\", \"cultural appropriation\"],\n",
    "                \"replacement\": \"skilled professional\",\n",
    "                \"explanation\": \"Uses casual language that may be inappropriate and culturally insensitive\"\n",
    "            },\n",
    "            \"crush targets\": {\n",
    "                \"categories\": [\"aggressive language\", \"toxic culture\"],\n",
    "                \"replacement\": \"achieve sales goals\",\n",
    "                \"explanation\": \"Promotes aggressive behavior and may indicate toxic work environment\"\n",
    "            },\n",
    "            \"long hours\": {\n",
    "                \"categories\": [\"work-life balance\", \"indirect discrimination\"],\n",
    "                \"replacement\": \"flexible schedule based on project needs\",\n",
    "                \"explanation\": \"May discriminate against caregivers and promote unhealthy work-life balance\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca32d11-16e4-429e-9599-f09796006783",
   "metadata": {},
   "source": [
    "What we want to achieve at steps 1 and 2:\n",
    "\n",
    "Enhanced bias dictionary to:\n",
    "\n",
    " - Include multiple categories per term\n",
    " - Highlight compounding effects\n",
    " - Provide more detailed explanations\n",
    "\n",
    "Improved prompt to:\n",
    "\n",
    " - Explicitly look for multiple types of discrimination\n",
    " - Consider how different biased terms interact\n",
    " - Assess compounding effects\n",
    " - Evaluate legal risks\n",
    "\n",
    "Enhanced report generation to show:\n",
    "\n",
    " - Detailed category-wise analysis\n",
    " - Compounding effects of multiple biases\n",
    " - Risk assessment\n",
    " - More comprehensive suggestions\n",
    " \n",
    "**What we expect for output?** The script to better detect how different types of discrimination overlap and compound each other in the job description. For example, it will recognize that:\n",
    "\n",
    " - \"young, energetic\" creates multiple layers of age discrimination\n",
    " - \"long hours\" compounds with age discrimination to create additional barriers\n",
    " - \"crush targets\" and \"ninja\" add unprofessional and aggressive language issues\n",
    " - The overall combination suggests potential legal risks and toxic culture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d14aa-12ce-45ae-900f-d93bf8d16dfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Use Gemini API to provide analysis on inclusivity context\n",
    "\n",
    "The key aspects of step 2 is that:\n",
    " - Split the prompt into two parts:\n",
    "   - Initial system prompt (`_create_initial_prompt`) that sets up the analysis framework\n",
    "   - Analysis prompt (`_create_analysis_prompt`) for each specific job description\n",
    " - Modified `analyze_job_description` to:\n",
    "   - Maintain conversation context\n",
    "   - Handle the multi-turn conversation format\n",
    "   - Store and use previous context for better analysis\n",
    " - Added `analyze_multiple_descriptions` method to:\n",
    "   - Process multiple job descriptions while maintaining context\n",
    "   - Return analyses for all descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbebdd-6e74-4478-a2b6-462f4da12b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_initial_prompt(self) -> str:\n",
    "    \"\"\"Create the initial system prompt explaining the task.\"\"\"\n",
    "    bias_terms_json = json.dumps(self.bias_dict, indent=2)\n",
    "\n",
    "    return f\"\"\"You are a job description analyzer specialized in detecting discriminatory language.\n",
    "    You will analyze job descriptions using these predefined problematic terms and categories:\n",
    "\n",
    "    {bias_terms_json}\n",
    "\n",
    "    For each job description, provide analysis in this JSON format:\n",
    "    {{\n",
    "        \"flagged_terms\": [\n",
    "            {{\n",
    "                \"term\": \"exact problematic phrase\",\n",
    "                \"categories\": [\"list\", \"of\", \"discrimination\", \"categories\"],\n",
    "                \"context\": \"full sentence containing the term\",\n",
    "                \"explanation\": \"detailed explanation of why this is problematic\",\n",
    "                \"suggestion\": \"specific replacement text\",\n",
    "                \"severity\": \"number 1-5, where 5 is most severe\",\n",
    "                \"compounding_effects\": \"explanation of how this term combines with others\"\n",
    "            }}\n",
    "        ],\n",
    "        \"discrimination_score\": \"number 0-10\",\n",
    "        \"confidence_level\": \"number 0-1\",\n",
    "        \"discrimination_categories\": {{\n",
    "            \"age_discrimination\": {{\n",
    "                \"count\": \"number of instances\",\n",
    "                \"severity\": \"average severity 1-5\",\n",
    "                \"terms\": [\"list of terms\"]\n",
    "            }},\n",
    "            \"unprofessional_language\": {{\n",
    "                \"count\": \"number of instances\",\n",
    "                \"severity\": \"average severity 1-5\",\n",
    "                \"terms\": [\"list of terms\"]\n",
    "            }},\n",
    "            \"work_life_balance\": {{\n",
    "                \"count\": \"number of instances\",\n",
    "                \"severity\": \"average severity 1-5\",\n",
    "                \"terms\": [\"list of terms\"]\n",
    "            }},\n",
    "            \"aggressive_language\": {{\n",
    "                \"count\": \"number of instances\",\n",
    "                \"severity\": \"average severity 1-5\",\n",
    "                \"terms\": [\"list of terms\"]\n",
    "            }}\n",
    "        }},\n",
    "        \"compounding_effects_summary\": \"explanation of how multiple biased terms interact\",\n",
    "        \"overall_risk_assessment\": \"analysis of legal and ethical risks\",\n",
    "        \"improved_description\": \"rewritten job description removing all biased language\"\n",
    "    }}\"\"\"\n",
    "\n",
    "def _create_analysis_prompt(self, job_description: str) -> str:\n",
    "    \"\"\"Create the prompt for analyzing a specific job description.\"\"\"\n",
    "    return f\"\"\"Analyze this job description for discriminatory language, considering all previous guidelines:\n",
    "\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    Provide your analysis in the specified JSON format.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808991c-a2b8-4282-b7c9-99e44ac26eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_job_description(self, job_description: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze a job description for bias and discrimination using conversation history.\"\"\"\n",
    "    try:\n",
    "        # Start new conversation if this is the first analysis\n",
    "        if not self.messages:\n",
    "            initial_prompt = self._create_initial_prompt()\n",
    "            self.messages = [\n",
    "                {'role': 'user', 'parts': [initial_prompt]}\n",
    "            ]\n",
    "            response = self.model.generate_content(self.messages)\n",
    "            self.messages.append(response.candidates[0].content)\n",
    "\n",
    "        # Add the job description analysis request\n",
    "        analysis_prompt = self._create_analysis_prompt(job_description)\n",
    "        self.messages.append({'role': 'user', 'parts': [analysis_prompt]})\n",
    "\n",
    "        # Get the analysis\n",
    "        response = self.model.generate_content(self.messages)\n",
    "\n",
    "        # Add the response to conversation history\n",
    "        self.messages.append(response.candidates[0].content)\n",
    "\n",
    "        # Parse and return the analysis\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Analysis failed: {str(e)}\",\n",
    "            \"flagged_terms\": [],\n",
    "            \"discrimination_score\": 0,\n",
    "            \"confidence_level\": 0,\n",
    "            \"discrimination_categories\": {\n",
    "                \"age_discrimination\": {\"count\": 0, \"severity\": 0, \"terms\": []},\n",
    "                \"unprofessional_language\": {\"count\": 0, \"severity\": 0, \"terms\": []},\n",
    "                \"work_life_balance\": {\"count\": 0, \"severity\": 0, \"terms\": []},\n",
    "                \"aggressive_language\": {\"count\": 0, \"severity\": 0, \"terms\": []}\n",
    "            },\n",
    "            \"compounding_effects_summary\": \"Analysis failed\",\n",
    "            \"overall_risk_assessment\": \"Analysis failed\",\n",
    "            \"improved_description\": job_description\n",
    "        }\n",
    "\n",
    "async def analyze_multiple_descriptions(self, descriptions: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Analyze multiple job descriptions while maintaining conversation context.\"\"\"\n",
    "    results = []\n",
    "    for description in descriptions:\n",
    "        analysis = await self.analyze_job_description(description)\n",
    "        results.append(analysis)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8331a2-b8df-460e-b047-be712669d1d5",
   "metadata": {},
   "source": [
    "Benefits of these changes:\n",
    "\n",
    " - Better context awareness across multiple analyses\n",
    " - More consistent analysis patterns\n",
    " - Potential for improved detection of subtle biases\n",
    " - Better handling of similar patterns across different job descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b318513-7134-474a-8195-56b05b318c35",
   "metadata": {},
   "source": [
    "### Step 3: Flag and recommend alternatives\n",
    "\n",
    "The script `job_bias_detector_args.py` is to accept job descriptions from command line arguments instead of hardcoding them, and includes several improvements:\n",
    "\n",
    " - Added command-line argument parsing using argparse\n",
    " - Supports two ways to input job descriptions: (1) Directly as command-line arguments, or (2) Through a text file (one description per line)\n",
    " - Added customizable output directory option\n",
    " - Improved error handling\n",
    " - Added helpful usage examples in the help text\n",
    "\n",
    "#### 3.1. Analyze single job description\n",
    "The command below not only generates analysis in the standard output console, but also in the analysis report logs, by default, `bias_analysis_reports/job_analysis_report_[i].txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4c6a2a-c435-4d11-9730-cd5f7f430a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Report 1:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 8/10\n",
      "                Confidence Level: 95.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "Age Discrimination:\n",
      "  Instances: 2\n",
      "  Average Severity: 4.5/5\n",
      "  Problematic Terms: young, energetic\n",
      "\n",
      "Unprofessional Language:\n",
      "  Instances: 0\n",
      "  Average Severity: 0/5\n",
      "  Problematic Terms: \n",
      "\n",
      "Work Life Balance:\n",
      "  Instances: 1\n",
      "  Average Severity: 4/5\n",
      "  Problematic Terms: long hours\n",
      "\n",
      "Aggressive Language:\n",
      "  Instances: 0\n",
      "  Average Severity: 0/5\n",
      "  Problematic Terms: \n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Flagged Term: young\n",
      "Categories: age discrimination, direct discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 5/5\n",
      "Explanation: Directly discriminates against older workers and violates age discrimination laws.\n",
      "Compounding Effects: This term, along with \"energetic\" and \"long hours\", creates a compounded discriminatory effect against older workers and those with family responsibilities.\n",
      "Suggested Replacement: motivated\n",
      "\n",
      "Flagged Term: energetic\n",
      "Categories: age discrimination, indirect discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 4/5\n",
      "Explanation: Often used as coded language for age discrimination and may discourage older applicants.\n",
      "Compounding Effects: Reinforces the age bias along with \"young\" and potentially suggests an expectation of a fast-paced work environment that might not be suitable for all ages.\n",
      "Suggested Replacement: enthusiastic\n",
      "\n",
      "Flagged Term: long hours\n",
      "Categories: work-life balance, indirect discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 4/5\n",
      "Explanation: May discriminate against caregivers and promote unhealthy work-life balance.\n",
      "Compounding Effects: Combined with \"young\" and \"energetic\", this phrase further marginalizes older workers and those with caregiving responsibilities who may have less flexibility in their schedules.\n",
      "Suggested Replacement: flexible schedule based on project needs\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "The combined use of \"young\", \"energetic\", and \"long hours\" creates a strong bias against older workers, those with family responsibilities, and individuals seeking a healthy work-life balance. This combination paints a picture of a demanding work environment that may not be inclusive of diverse needs and lifestyles.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description poses significant legal risks due to potential age discrimination and violations related to work-life balance. It may also negatively impact the company's image and deter qualified candidates from applying.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "We need a motivated and enthusiastic salesperson with a flexible schedule based on project needs.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python job_bias_detector_args.py \"We need a young, energetic salesperson who can work long hours!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418e27e-a741-4537-96de-6ff3dd0d8ea6",
   "metadata": {},
   "source": [
    "#### 3.2. Analyze multiple job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d24390-946e-44ec-a1f0-a7d278b0d3de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Report 1:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 8/10\n",
      "                Confidence Level: 95.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "Age Discrimination:\n",
      "  Instances: 2\n",
      "  Average Severity: 4.5/5\n",
      "  Problematic Terms: young, energetic\n",
      "\n",
      "Work Life Balance:\n",
      "  Instances: 1\n",
      "  Average Severity: 4/5\n",
      "  Problematic Terms: long hours\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Flagged Term: young\n",
      "Categories: age discrimination, direct discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 5/5\n",
      "Explanation: Directly discriminates against older workers and violates age discrimination laws.\n",
      "Compounding Effects: This term contributes significantly to the overall discriminatory nature of the job description.\n",
      "Suggested Replacement: motivated\n",
      "\n",
      "Flagged Term: energetic\n",
      "Categories: age discrimination, indirect discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 4/5\n",
      "Explanation: Often used as coded language for age discrimination and may discourage older applicants.\n",
      "Compounding Effects: This term reinforces the age discrimination introduced by the word 'young'.\n",
      "Suggested Replacement: enthusiastic\n",
      "\n",
      "Flagged Term: long hours\n",
      "Categories: work-life balance, indirect discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 4/5\n",
      "Explanation: May discriminate against caregivers and promote unhealthy work-life balance.\n",
      "Compounding Effects: While not inherently discriminatory, the expectation of 'long hours' can disproportionately affect certain groups, compounding the existing age discrimination.\n",
      "Suggested Replacement: flexible schedule based on project needs\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "The combination of 'young,' 'energetic,' and 'long hours' creates a strong bias against older workers and those with family responsibilities.  The requirement of 'long hours' exacerbates the existing age discrimination, suggesting a demanding work environment that might not be suitable for all.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description presents a high legal risk due to the explicit age discrimination. It also carries ethical concerns, fostering an exclusionary environment.  Revision is crucial to mitigate these risks.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "We need a motivated and enthusiastic salesperson with a flexible schedule based on project needs.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analysis Report 2:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 0/10\n",
      "                Confidence Level: 99.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "No compounding effects detected.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description does not appear to contain any discriminatory language.  The requirements of flexible hours and handling high-pressure situations are generally acceptable, although it is important to ensure that 'flexible hours' are implemented fairly and not used to exploit employees.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "Must be willing to work flexible hours and handle high-pressure situations.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analysis Report 3:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 6/10\n",
      "                Confidence Level: 90.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "Unprofessional Language:\n",
      "  Instances: 1\n",
      "  Average Severity: 3/5\n",
      "  Problematic Terms: rockstar\n",
      "\n",
      "Aggressive Language:\n",
      "  Instances: 1\n",
      "  Average Severity: 4/5\n",
      "  Problematic Terms: crush sales targets\n",
      "\n",
      "Work Life Balance:\n",
      "  Instances: 1\n",
      "  Average Severity: 4/5\n",
      "  Problematic Terms: long hours\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Flagged Term: rockstar\n",
      "Categories: unprofessional language\n",
      "Context: \"The ideal candidate should be a rockstar who can crush sales targets and work long hours.\"\n",
      "Severity: 3/5\n",
      "Explanation: Using terms like 'rockstar' can create a biased perception and may not translate professionally across all cultures or demographics. It can also contribute to a potentially exclusionary work environment.\n",
      "Compounding Effects: While not overtly discriminatory, this term contributes to an overall informal tone that may discourage some applicants.\n",
      "Suggested Replacement: high-achieving professional\n",
      "\n",
      "Flagged Term: crush sales targets\n",
      "Categories: aggressive language, toxic culture\n",
      "Context: \"The ideal candidate should be a rockstar who can crush sales targets and work long hours.\"\n",
      "Severity: 4/5\n",
      "Explanation: Promotes aggressive behavior and may indicate a potentially toxic work environment.  This language may be off-putting to some candidates.\n",
      "Compounding Effects: Combined with 'rockstar,' this phrase amplifies the impression of a potentially aggressive and high-pressure work environment.\n",
      "Suggested Replacement: achieve sales goals\n",
      "\n",
      "Flagged Term: long hours\n",
      "Categories: work-life balance, indirect discrimination\n",
      "Context: \"The ideal candidate should be a rockstar who can crush sales targets and work long hours.\"\n",
      "Severity: 4/5\n",
      "Explanation: May discriminate against caregivers and promote unhealthy work-life balance.\n",
      "Compounding Effects: This adds to the potentially discriminatory nature by implying an expectation of excessive work hours, further reinforcing the impression of a demanding and potentially unbalanced work environment.\n",
      "Suggested Replacement: be flexible with their working hours based on project needs\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "The combination of 'rockstar,' 'crush sales targets,' and 'long hours' creates a perception of a potentially aggressive, high-pressure, and unbalanced work environment. This could discourage applicants who prioritize work-life balance or prefer a less intense work setting.  While not directly discriminatory, this combination of terms can contribute to an exclusionary environment.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description presents a moderate ethical risk due to the use of unprofessional and aggressive language. While not legally discriminatory, this language could deter qualified candidates and foster a negative perception of the company culture. Revising the description is recommended to mitigate these risks.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "The ideal candidate should be a high-achieving professional who can achieve sales goals and be flexible with their working hours based on project needs.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python job_bias_detector_args.py \"We need a young, energetic salesperson who can work long hours!\" \"Must be willing to work flexible hours and handle high-pressure situations.\" \"The ideal candidate should be a rockstar who can crush sales targets and work long hours.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbced4-2c66-4de9-997a-938c004b30cf",
   "metadata": {},
   "source": [
    "#### 3.3. Analyze job descriptions from a file\n",
    "\n",
    "If you're using a file to input job descriptions, format it like this:\n",
    "```\n",
    "We need a young, energetic salesperson who can work long hours!\n",
    "Looking for a fresh graduate with 2-3 years of experience.\n",
    "We are looking for a young, energetic salesperson to join our team.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0a1881-f92a-4838-bbb0-fd19c4a67e76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Report 1:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 8/10\n",
      "                Confidence Level: 95.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "Age Discrimination:\n",
      "  Instances: 2\n",
      "  Average Severity: 4.5/5\n",
      "  Problematic Terms: young, energetic\n",
      "\n",
      "Work Life Balance:\n",
      "  Instances: 1\n",
      "  Average Severity: 3/5\n",
      "  Problematic Terms: long hours\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Flagged Term: young\n",
      "Categories: age discrimination, direct discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 5/5\n",
      "Explanation: Directly discriminates against older workers and violates age discrimination laws. Using \"young\" as a qualifier unfairly limits the applicant pool.\n",
      "Compounding Effects: Combined with \"energetic\" and \"long hours,\" this term reinforces age bias.\n",
      "Suggested Replacement: motivated\n",
      "\n",
      "Flagged Term: energetic\n",
      "Categories: age discrimination, indirect discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 4/5\n",
      "Explanation: Often used as coded language for age discrimination, potentially discouraging older applicants.\n",
      "Compounding Effects: Reinforces the age bias created by \"young\" and \"long hours.\"\n",
      "Suggested Replacement: enthusiastic\n",
      "\n",
      "Flagged Term: long hours\n",
      "Categories: work-life balance, indirect discrimination\n",
      "Context: \"We need a young, energetic salesperson who can work long hours!\"\n",
      "Severity: 3/5\n",
      "Explanation: May discriminate against caregivers and promote unhealthy work-life balance.\n",
      "Compounding Effects: Exacerbates the discriminatory implications of \"young\" and \"energetic.\"\n",
      "Suggested Replacement: flexible schedule based on project needs\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "The combination of \"young,\" \"energetic,\" and \"long hours\" creates a strong bias towards younger applicants and against older workers or those with caregiving responsibilities, potentially violating age discrimination laws and promoting unhealthy work practices.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description poses a high legal risk due to explicit age discrimination (\"young\") and the potential for claims of indirect discrimination based on age and family status (\"energetic\" and \"long hours\"). It also presents ethical concerns regarding inclusivity and fair hiring practices.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "We need a motivated and enthusiastic salesperson who can work a flexible schedule based on project needs.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analysis Report 2:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 4/10\n",
      "                Confidence Level: 70.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "Age Discrimination:\n",
      "  Instances: 1\n",
      "  Average Severity: 4/5\n",
      "  Problematic Terms: fresh graduate\n",
      "\n",
      "Experience Discrimination:\n",
      "  Instances: 1\n",
      "  Average Severity: 4/5\n",
      "  Problematic Terms: fresh graduate\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Flagged Term: fresh graduate\n",
      "Categories: age discrimination, indirect discrimination, experience discrimination\n",
      "Context: \"Looking for a fresh graduate with 2-3 years of experience.\"\n",
      "Severity: 4/5\n",
      "Explanation: While not explicitly discriminatory, \"fresh graduate\" often implies a preference for recent graduates, potentially disadvantaging older workers with more experience.  The conflicting requirement of 2-3 years of experience further highlights this issue, suggesting a narrow target demographic.\n",
      "Compounding Effects: The contradictory nature of seeking a 'fresh graduate' with significant experience exacerbates the potential for discrimination.\n",
      "Suggested Replacement: entry-level professional\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "The conflicting requirements of being a \"fresh graduate\" while simultaneously possessing 2-3 years of experience create ambiguity and raise concerns about potential age and experience discrimination. This confusing language likely unintentionally excludes qualified candidates.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description presents a moderate legal risk due to the potential for indirect age and experience discrimination. Although \"fresh graduate\" is not explicitly discriminatory, its combination with the experience requirement creates a contradictory message that may deter qualified candidates from applying.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "Looking for an entry-level professional with 2-3 years of experience.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analysis Report 3:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 7/10\n",
      "                Confidence Level: 90.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "Age Discrimination:\n",
      "  Instances: 2\n",
      "  Average Severity: 4.5/5\n",
      "  Problematic Terms: young, fresh-brained\n",
      "\n",
      "Unprofessional Language:\n",
      "  Instances: 2\n",
      "  Average Severity: 3.5/5\n",
      "  Problematic Terms: fresh-brained, sales ninja\n",
      "\n",
      "Cultural Appropriation:\n",
      "  Instances: 1\n",
      "  Average Severity: 3/5\n",
      "  Problematic Terms: sales ninja\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Flagged Term: young\n",
      "Categories: age discrimination, direct discrimination\n",
      "Context: \"We are looking for a young, fresh-brained sales ninja to join our team.\"\n",
      "Severity: 5/5\n",
      "Explanation: Explicitly discriminates against older workers based on age, violating age discrimination laws.  The term \"young\" suggests that older applicants need not apply.\n",
      "Compounding Effects: This term's discriminatory nature is amplified when combined with \"fresh-brained\" and \"sales ninja,\" creating a cumulative effect of ageism and unprofessionalism.\n",
      "Suggested Replacement: motivated\n",
      "\n",
      "Flagged Term: fresh-brained\n",
      "Categories: age discrimination, indirect discrimination, unprofessional language\n",
      "Context: \"We are looking for a young, fresh-brained sales ninja to join our team.\"\n",
      "Severity: 4/5\n",
      "Explanation: Although not explicitly discriminatory, \"fresh-brained\" implies a preference for recent graduates or younger individuals, potentially disadvantaging older applicants with more experience.  Furthermore, it is unprofessional jargon.\n",
      "Compounding Effects: This term reinforces the ageist implications of \"young\" and adds to the overall unprofessional tone set by \"sales ninja.\"\n",
      "Suggested Replacement: innovative\n",
      "\n",
      "Flagged Term: sales ninja\n",
      "Categories: unprofessional language, cultural appropriation\n",
      "Context: \"We are looking for a young, fresh-brained sales ninja to join our team.\"\n",
      "Severity: 3/5\n",
      "Explanation: This term uses casual and potentially culturally insensitive language that may not resonate with all applicants. It also projects an unprofessional image of the company.\n",
      "Compounding Effects: Combined with \"young\" and \"fresh-brained,\" this term contributes to a perception of the ideal candidate as someone young, inexperienced, and potentially aggressive in sales tactics.\n",
      "Suggested Replacement: skilled sales professional\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "The combination of \"young,\" \"fresh-brained,\" and \"sales ninja\" creates a trifecta of potentially discriminatory and unprofessional language.  It suggests a preference for younger applicants, potentially excluding qualified candidates based on age and contributing to a negative perception of the company culture.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description presents a significant legal risk due to the explicit age discrimination (\"young\") and a moderate risk of deterring qualified candidates due to unprofessional language. Ethically, it promotes an exclusionary image and may discourage older, more experienced applicants from applying.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "We are looking for a motivated, innovative, and skilled sales professional to join our team.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python job_bias_detector_args.py -f job_descriptions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89ea80-e31b-4179-bde2-e55113f1793c",
   "metadata": {},
   "source": [
    "#### 3.4. Specify custom output directory\n",
    "The command below not only generates analysis in the standard output console, but also in the analysis report logs, into the customized folder, in this case, `custom_reports/job_analysis_report_[i].txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce26300c-90e7-4f69-a6c9-05118654645f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Report 1:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 0/10\n",
      "                Confidence Level: 99.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "No compounding effects detected as no biased terms were found.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description title, while referencing bias, doesn't itself contain any discriminatory language.  However, it highlights the importance of careful wording in job descriptions to avoid unintended bias.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "Biases in Job Descriptions\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Analysis Report 2:\n",
      "Job Description Bias Analysis Report\n",
      "                ================================================================================\n",
      "\n",
      "                OVERALL METRICS\n",
      "                ----------------------------------------\n",
      "                Discrimination Score: 5/10\n",
      "                Confidence Level: 85.0%\n",
      "\n",
      "                DISCRIMINATION CATEGORIES ANALYSIS\n",
      "                ----------------------------------------\n",
      "\n",
      "Age Discrimination:\n",
      "  Instances: 1\n",
      "  Average Severity: 3/5\n",
      "  Problematic Terms: fresh graduate\n",
      "\n",
      "Aggressive Language:\n",
      "  Instances: 1\n",
      "  Average Severity: 4/5\n",
      "  Problematic Terms: crush targets\n",
      "\n",
      "DETAILED TERM ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "Flagged Term: fresh graduate\n",
      "Categories: age discrimination, indirect discrimination\n",
      "Context: \"Looking for a fresh graduate with 2-3 years of experience who can crush targets and work under pressure.\"\n",
      "Severity: 3/5\n",
      "Explanation: While not explicitly stating an age, \"fresh graduate\" often implies a preference for recent graduates, which can indirectly discriminate against older applicants with more experience.\n",
      "Compounding Effects: Combined with the requirement of 2-3 years of experience, this creates a confusing and potentially contradictory requirement that may discourage applicants.\n",
      "Suggested Replacement: entry-level professional\n",
      "\n",
      "Flagged Term: crush targets\n",
      "Categories: aggressive language, toxic culture\n",
      "Context: \"Looking for a fresh graduate with 2-3 years of experience who can crush targets and work under pressure.\"\n",
      "Severity: 4/5\n",
      "Explanation: Promotes aggressive behavior and may indicate a toxic work environment.\n",
      "Compounding Effects: The combination of \"crush targets\" and \"work under pressure\" paints a picture of a high-pressure, potentially stressful work environment.\n",
      "Suggested Replacement: achieve sales goals\n",
      "\n",
      "COMPOUNDING EFFECTS SUMMARY\n",
      "----------------------------------------\n",
      "The combination of \"fresh graduate\" with \"crush targets\" and \"work under pressure\" suggests a potentially demanding and aggressive work environment that may be particularly unappealing to older or more experienced candidates.\n",
      "\n",
      "RISK ASSESSMENT\n",
      "----------------------------------------\n",
      "This job description presents a moderate legal risk due to the potential for indirect age discrimination.  It also raises ethical concerns regarding the promotion of a potentially aggressive and high-pressure work environment.\n",
      "\n",
      "IMPROVED JOB DESCRIPTION\n",
      "----------------------------------------\n",
      "Looking for an entry-level professional with 2-3 years of experience who can achieve sales goals and perform effectively in a dynamic environment.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python job_bias_detector_args.py -o custom_reports \"Biases in Job Descriptions\" \"Looking for a fresh graduate with 2-3 years of experience who can crush targets and work under pressure.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa87c81-a9cb-48ef-9297-2b78d81aad31",
   "metadata": {},
   "source": [
    "#### 3.5. See help and usage information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6bd6ef-a395-4e3f-a97d-678b52232945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: job_bias_detector_args.py [-h] [-f FILE] [-o OUTPUT_DIR]\n",
      "                                 [descriptions ...]\n",
      "\n",
      "Analyze job descriptions for potential bias and discrimination.\n",
      "\n",
      "positional arguments:\n",
      "  descriptions          Job descriptions to analyze (as quoted strings)\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -f FILE, --file FILE  File containing job descriptions (one per line)\n",
      "  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n",
      "                        Directory to store analysis reports (default:\n",
      "                        bias_analysis_reports)\n",
      "\n",
      "Examples:\n",
      "    python script.py \"Job description 1\" \"Job description 2\"\n",
      "    python script.py -f job_descriptions.txt\n",
      "    python script.py -o custom_output_dir \"Job description 1\"\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "!python job_bias_detector_args.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6fcb75-66c9-461c-9f7c-a9c7e7daef68",
   "metadata": {},
   "source": [
    "### Step 4. Feedback UI\n",
    "\n",
    "At this step, we will create a Jupyter Lab UI component that integrates with the bias detection system and includes feedback collection. To complement this UI component, we'll also need a training feedback processor that can help improve the model's suggestions based on user feedback (as stated in step 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a9b2bfd-5b59-49f8-be67-704b00a9209a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from job_bias_detector import JobBiasDetector\n",
    "from job_bias_ui import JobBiasAnalyzerUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93cbee52-2c4b-4ec5-9b14-165cf3d23212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Information ===\n",
      "Python version: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:47:54) [MSC v.1941 64 bit (AMD64)]\n",
      "IPython version: 8.28.0\n",
      "Working directory: C:\\Users\\pengc\\Downloads\\ML_Project\\llm_notebooks\\llm_workplace\\gemini_usage\n",
      "\n",
      "=== Initializing UI ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca9697f488d44a7abc38c76b0c5b109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Job Description Bias Analyzer</h2>'), HTML(value=\"<p>Paste your job description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage example with enhanced debug output\n",
    "print(\"=== Environment Information ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"IPython version: {IPython.__version__}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(\"\\n=== Initializing UI ===\")\n",
    "\n",
    "try:\n",
    "    analyzer_ui = JobBiasAnalyzerUI()\n",
    "    analyzer_ui.display()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize UI: {str(e)}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0674fa-bdbc-4241-b925-5b1a4f8d30d2",
   "metadata": {},
   "source": [
    "Users can:\n",
    "\n",
    " - Paste their job descriptions\n",
    " - Click \"Analyze\" to get results\n",
    " - Provide feedback using radio buttons for each suggestion\n",
    " - See real-time results and explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464e1ef-d45a-4ee4-b690-203a5a61f0e2",
   "metadata": {},
   "source": [
    "### Step 5. Feedback Loop\n",
    "\n",
    "To analyze feedback and improve the model, we will need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59ff0d80-b937-490f-ac14-58ba3935af47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Detection Model Improvement Report\n",
      "==================================================\n",
      "\n",
      "Terms Needing Improvement:\n",
      "\n",
      "Successful Suggestions:\n"
     ]
    }
   ],
   "source": [
    "from feedback_processor import print_improvement_report\n",
    "print_improvement_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f8320-372e-43d8-89eb-03a4354a3965",
   "metadata": {},
   "source": [
    "In steps 4 and 5, we have implemented these key features:\n",
    "\n",
    " - Clean, intuitive UI with textarea input and analysis button\n",
    " - Real-time analysis results display\n",
    " - Feedback collection through radio buttons\n",
    " - SQLite database for feedback storage\n",
    " - Feedback analysis system for model improvement\n",
    " - Context analysis to understand when suggestions work best\n",
    " - Detailed reporting system for model refinement\n",
    "\n",
    "The system stores all feedback in a SQLite database, which can be used to:\n",
    "\n",
    " - Track which suggestions are most helpful\n",
    " - Identify patterns in different contexts\n",
    " - Generate reports for model improvement\n",
    " - Provide data for retraining or fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d0926-73a5-4df1-8fba-14b7699d4df7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ethical Considerations and Implementations\n",
    "\n",
    "### Ethical AI Implementation\n",
    "The tool addresses ethical considerations through:\n",
    "\n",
    " - Transparent explanation of flagged terms and reasoning\n",
    " - User feedback collection to improve accuracy and reduce false positives\n",
    " - Privacy-conscious design that doesn't store sensitive job description data\n",
    " - Regular updates to bias detection rules based on evolving social understanding\n",
    " - Consideration of industry-specific contexts and requirements\n",
    "\n",
    "### Intersectional Approach\n",
    "The analyzer considers multiple dimensions of bias:\n",
    "\n",
    " - Gender-coded language and stereotypes\n",
    " - Age-related bias in experience requirements\n",
    " - Cultural and racial bias in language\n",
    " - Accessibility and disability considerations\n",
    " - Educational and socioeconomic barriers\n",
    "\n",
    "\n",
    "## Future Enhancements\n",
    "Due to time restrictions, the tool is accomplished with basic functionalities. For future development, the planned improvements would include:\n",
    "\n",
    " - Integration with Gemini for more sophisticated language understanding\n",
    " - Web-based interface for broader accessibility\n",
    " - API endpoints for integration with applicant tracking systems\n",
    " - Enhanced reporting and analytics features\n",
    " - Support for multiple languages\n",
    " - Integration with HR policy compliance frameworks\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Job Bias Analyzer represents a practical, scalable solution for promoting gender equality in the workplace by addressing the fundamental issue of bias in job descriptions. By making job postings more inclusive and welcoming, it helps create equal opportunities for women in the workforce, provide a more diverse and equitable work environment, and contributes to the broader goals of gender equality and women's empowerment.\n",
    "\n",
    "**Author:** [Chunming Peng](https://github.com/PGCHM)\n",
    "\n",
    "**GitHub Repo:** https://github.com/PGCHM/job_bias_detector\n",
    "\n",
    "**Footnote Image Provided by** [Vector Stock](https://www.vectorstock.com/royalty-free-vector/)\n",
    "\n",
    "<img src=\"img/yes-women-can-symbol-female-power-woman-rights-vector-24758529.jpg\" alt=\"Yes Women Can!\" style=\"width: 1400px; height: 400px\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
